{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LSYFfZLwoCC8",
        "FP3LicBYnuZS",
        "OPg4ysM5em0M",
        "G9D_u2K1mNXh",
        "qAewtNLzomDD",
        "gef-HT1zr0sg"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valeriia-nekrasova/compling/blob/main/%D0%A2%D1%80%D0%B5%D1%82%D1%8C%D0%B5_%D0%B4%D0%BE%D0%BC%D0%B0%D1%88%D0%BD%D0%B5%D0%B5_%D0%B7%D0%B0%D0%B4%D0%B0%D0%BD%D0%B8%D0%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Коспект-шпаргалка по SMT**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dr66NBAFwgXn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Общая информация**"
      ],
      "metadata": {
        "id": "LSYFfZLwoCC8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Что такое SMT?*\n",
        "\n",
        "*   Это разновидность машинного перевода, которая основа на поиске наиболее правдободобного перервода для языка на основе параллельного корпуса предложений исходного языка и языка, на который переводится предложение.\n",
        "*   Наиболее точным переводом считается тот, при котором максимизируется вероятность употребления перевода слов в языке У при переводе с языка X (предложение на языке Х надо перевести на язык У)\n",
        "*   Для начала строится языковая модель вероятности для У\n",
        "*   Затем вычисляются допустимые пары Х-У с помощью статистических совпадений выравненных фраз в параллельном корпусе, который построен на вероятность перевода в языке Х при условии языка У\n",
        "*   Далее максимизируется результат вычисления вероятность для языка У и корпусе, который построен на вероятность перевода в языке Х при условии языка У\n",
        "\n",
        "\n",
        "**Компоненты SMT**\n",
        "\n",
        "*N-граммная языковая модель* - моделируем вероятностное распределение конструкций на уровне слов или фраз в языке Y\n",
        "\n",
        "*Модель перевода (t-model*) - собираем статистику соотвествий фраз в паралелльном корпусе, ищем переводческие соответствия X - Y и моделируем их с помощью теории вероятности:\n",
        "\n",
        "Допускаем, что любое предложение языка Y может быть \"искаженной\" версией некой фразы на языке X\n",
        "\n",
        "Ищем наиболее правдоподобные соответствия X - Y\n",
        "\n",
        "*Декодер* - ищем наиболее грамматичные и лексически правдоподобные результаты, отбираем среди гипотез один результат"
      ],
      "metadata": {
        "id": "3q5THlzvn5L_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Первый этап - препроцессинг**"
      ],
      "metadata": {
        "id": "FP3LicBYnuZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Перед началом работы необходимо загрузить библиотеку  https://scikit-learn.org/stable/ **\n",
        "Источник данных, на которых можно обучать модели для машинного перевода: OPUS Corpora. Это открытая коллекция параллельных корпусов текстов на различных языках: книги, статьи, переводы и другие типы текстов, которые могут быть использованы для обучения и оценки статистических моделей машинного перевода.\n",
        "- На данном этапе необходимо выполнить загрузку данных:\n",
        "извлечь данные из архива ( использовать модуль Python `tarfile` https://docs.python.org/3/library/tarfile.html ). Посмотреть список распакованных файлов можно с помощью shell команды ls\n",
        "```\n",
        "with tarfile.open('НАЗВАНИЕ_АРХИВА', 'r:ТИП_АРХИВА') as tar:\n",
        "  tar.extractall()\n",
        "```\n",
        "- На основании извлеченных данных создаем выборки. Так делаем для каждого языка\n",
        "```\n",
        "with open('НАЗВАНИЕ_РАСПАКОВАННОГО_ФАЙЛА', 'r') as f:\n",
        "  text = f.read().split('\\n')[:-1]\n",
        "```\n",
        "- разделяем полученные выборки на обучающую и на тестовую с помощью функции `train_test_split` библиотеки `sklearn`. `array1` и `array2` - созданные ранее выборки\n",
        "```\n",
        "X_train, X_test, y_train, y_test = train_test_split(array1, array2)\n",
        "```\n",
        "На данном этапе также можно провести необходимую подготовку данных:\n",
        "*   Отчистить данные\n",
        "*   Выполнить токенизацию ( разделить каждое предложение по словам )\n",
        "```\n",
        "def tokenize(sentences):\n",
        "  return [sentence.split() for sentence in sentences]\n",
        "```\n",
        "*   Также можно создать словарь для каждого языка: объединить слова со всех предложений\n",
        "\n"
      ],
      "metadata": {
        "id": "9f2vXUZeXf7E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Второй этап - модель SMT**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OPg4ysM5em0M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. IBM 1 Expectation-Maximization (t-model)**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G9D_u2K1mNXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для начала необходимо создать t-table - таблицу вероятности соответствия слов. На первом шаге заполняем таблицу для каждого слова обучающей выборки значением вероятности того, что случайное слово x_vocab соответсвует случайному слову y_vocab, равным:\n",
        "\n",
        "```\n",
        "uniform = 1 / (len(x_vocab) * len(y_vocab))\n",
        "```\n",
        "После этого задаем кол-во итераций обучений и запускаем цикл по количеству итераций\n",
        "\n",
        "В цикле необходимо выполнить 3 шага, каждый шаг - проход по обучающей выбоки в цикле\n",
        "\n",
        "```\n",
        " for i in range(len(X_train)):\n",
        "```\n",
        "- Шаг 0:\n",
        "  - создать слоты для подсчета статистики. Р(х|у) - условная вероятность совпадения в корпусе и Р(у) - статистическая языковая модель y.\n",
        "- Шаг 1:\n",
        "  - Необходимо посчитать статистику х. Для этого создаем  `total_stat = {}` и   \n",
        "   собираем предварительную статистику на основе данных x : увеличиваем значение статистики при обноружении совместной встречаемости. По факту проходим по каждому слову и накапливаем сумму `t[(word_x, word_y)]`\n",
        "  - После этого необходимо обновить данные для P(x|y) и P(y). Подсчитываем условную вероятность совпадений в корпусе (равномерное распределение / частотность x) и статистическую информации y (равномерное распределение / частотность x).\n",
        "```\n",
        "for word_x in X_train_tokens[i]:\n",
        "      for word_y in y_train_tokens[i]:\n",
        "        count[(word_x, word_y)] += t[(word_x, word_y)] / total_stat[word_x]\n",
        "        total[word_y] += t[(word_x, word_y)] / total_stat[word_x]\n",
        "```\n",
        "- Шаг 2: обновить t-table - для каждой пары х и у считаем величину: вероятность совпадения в корпусе / вероятность информации y\n",
        "```\n",
        "for word_x in X_train_tokens[i]:\n",
        "      for word_y in y_train_tokens[i]:\n",
        "        t[(word_x, word_y)] = count[(word_x, word_y)] / total[word_y]\n",
        "```"
      ],
      "metadata": {
        "id": "Nti3o-QjoY5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Биграммная модель**"
      ],
      "metadata": {
        "id": "qAewtNLzomDD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Прежде всего объединяем две выборки для обучения модели и создаем хранилище для биграмм\n",
        "```\n",
        "tokens = ' '.join(german).split()\n",
        "bigram_model = defaultdict(list)\n",
        "```\n",
        "Затем собираем все попарные совпадения: сохраняем в хранилище биграмм (слово, следующее слово)\n",
        "```\n",
        "for i in range(len(tokens)-1):\n",
        "    current_word = tokens[i]\n",
        "    next_word = tokens[i + 1]\n",
        "    bigram_model[current_word].append(next_word)\n",
        "```\n",
        "Затем необходимо задать количество шагов, по аналогии, как мы делали в t-table.\n",
        "Случайным образом выбираем токен с помощью модуля Python random и добавляем его в генерируемое предложение.\n",
        "```\n",
        "current_word = random.choice(tokens)\n",
        "generated_sentence = current_word\n",
        "```\n",
        "Затем на каждом шаге выбираем правдободобные варианты продолжения для текущего токена и опять случайным образом выбираем новый токен и добавляем его в генерируемое предложение.\n",
        "```\n",
        "next_word_options = model[current_word]\n",
        "current_word = random.choice(next_word_options)\n",
        "generated_sentence += ' '\n",
        "generated_sentence += current_word\n",
        "```\n",
        "В результате получаем сгенерированное заданным количеством шагов предложение\n"
      ],
      "metadata": {
        "id": "YYgQuaF1osGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Третий этап - оценка результатов**"
      ],
      "metadata": {
        "id": "gef-HT1zr0sg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Оценивать результаты будем с помощью модуля Python NLTK - https://www.nltk.org/api/nltk.translate.bleu_score.html\n",
        "\n",
        "Отсортируем `t-table` по убыванию правдоподобия, используя функцию `sorted` и установив `reverse = True`:\n",
        "`sorted_t = sorted(t.items(), key = lambda k:(k[1], k[0]), reverse = True)`\n",
        "Реализуем функцию поиска наиболее подходящего токена:\n",
        "```\n",
        "def translate(token):\n",
        "  for element in sorted_t:\n",
        "    if element[0][1] == token:\n",
        "      return element[0][0]\n",
        "```\n",
        "Теперь для каждого предложения из тестовой выборки получим его перевод: для каждого токена (слова)  из тестового предложения найдем наиболее подходящий перевод и запишем его в `translation`:\n",
        "```\n",
        "for sentence in y_test_tokens:\n",
        "  translation = []\n",
        "  for token in sentence:\n",
        "    translation.append(translate(token))\n",
        "```\n",
        "Наконец, с помощью функции `corpus_bleu` из `nltk.translate.bleu_score`  сравним что выдает наша модель на тестовой выборке и правильный перевод из тестовой выборки. В результате получим `bleu_score`: чем он выше, тем лучше работает модель.\n",
        "```\n",
        "reference = [X_test_tokens[0], X_test_tokens[1]]\n",
        "candidate = [translate(token) for token in y_test_tokens[0]]\n",
        "bleu_score = corpus_bleu(reference, candidate)\n",
        "```"
      ],
      "metadata": {
        "id": "Vz7OzAYbsJZl"
      }
    }
  ]
}